{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-commerce Sales Prediction - Exploratory Data Analysis\n",
    "\n",
    "This notebook contains comprehensive exploratory data analysis for the e-commerce sales prediction project. We'll analyze various aspects of the data to understand patterns and relationships that can help in predicting weekly sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3.12.0'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. WebSocket is not defined"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install git-lfs\n",
    "!git lfs install\n",
    "!sudo chmod -R 777 /workspace/big-data/.git/lfs/\n",
    "!git lfs pull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when, count, mean, stddev\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import project utilities\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalesPrediction_EDA\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv('../data/raw/BaSalam.products.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Number of records: {df.count():,}\")\n",
    "print(f\"Number of features: {len(df.columns)}\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions for the sales prediction project.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col, count, mean, stddev\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_missing_values(df):\n",
    "    \"\"\"\n",
    "    Plot missing values analysis.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "    \"\"\"\n",
    "    total_count = df.count()\n",
    "    missing_counts = []\n",
    "    \n",
    "    for column in df.columns:\n",
    "        missing_count = df.filter(col(column).isNull()).count()\n",
    "        missing_percentage = (missing_count / total_count) * 100\n",
    "        missing_counts.append({\n",
    "            'column': column,\n",
    "            'missing_percentage': missing_percentage\n",
    "        })\n",
    "    \n",
    "    missing_df = pd.DataFrame(missing_counts)\n",
    "    missing_df = missing_df.sort_values('missing_percentage', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(missing_df['column'], missing_df['missing_percentage'])\n",
    "    plt.xlabel('Missing Percentage')\n",
    "    plt.title('Missing Values Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_matrix(df, columns):\n",
    "    \"\"\"\n",
    "    Plot correlation matrix for specified columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        columns: List of column names to include in correlation matrix\n",
    "    \"\"\"\n",
    "    correlation_data = df.select(columns).toPandas()\n",
    "    correlation_matrix = correlation_data.corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def detect_outliers(df, column, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using z-score method.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        column: Column name to check for outliers\n",
    "        threshold: Z-score threshold for outlier detection\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with outlier indicators\n",
    "    \"\"\"\n",
    "    stats = df.select(\n",
    "        mean(col(column)).alias('mean'),\n",
    "        stddev(col(column)).alias('stddev')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    mean_val = stats['mean']\n",
    "    stddev_val = stats['stddev']\n",
    "    \n",
    "    return df.withColumn(\n",
    "        f'{column}_zscore',\n",
    "        (col(column) - mean_val) / stddev_val\n",
    "    ).withColumn(\n",
    "        f'{column}_is_outlier',\n",
    "        (col(f'{column}_zscore').abs() > threshold)\n",
    "    )\n",
    "\n",
    "def evaluate_regression_model(predictions, label_col=\"label\", prediction_col=\"prediction\"):\n",
    "    \"\"\"\n",
    "    Evaluate regression model performance.\n",
    "    \n",
    "    Args:\n",
    "        predictions: DataFrame with actual and predicted values\n",
    "        label_col: Name of the label column\n",
    "        prediction_col: Name of the prediction column\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate MSE\n",
    "    mse = predictions.select(\n",
    "        ((col(prediction_col) - col(label_col)) ** 2).alias(\"squared_error\")\n",
    "    ).agg({\"squared_error\": \"avg\"}).collect()[0][0]\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Calculate R-squared\n",
    "    total_variance = predictions.select(\n",
    "        (col(label_col) - predictions.select(mean(label_col)).collect()[0][0]) ** 2\n",
    "    ).agg({\"label\": \"sum\"}).collect()[0][0]\n",
    "    \n",
    "    residual_variance = predictions.select(\n",
    "        (col(prediction_col) - col(label_col)) ** 2\n",
    "    ).agg({prediction_col: \"sum\"}).collect()[0][0]\n",
    "    \n",
    "    r2 = 1 - (residual_variance / total_variance)\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = predictions.select(\n",
    "        (abs(col(prediction_col) - col(label_col))).alias(\"abs_error\")\n",
    "    ).agg({\"abs_error\": \"avg\"}).collect()[0][0]\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "        \"mae\": mae\n",
    "    }\n",
    "\n",
    "def plot_actual_vs_predicted(predictions, label_col=\"label\", prediction_col=\"prediction\"):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values.\n",
    "    \n",
    "    Args:\n",
    "        predictions: DataFrame with actual and predicted values\n",
    "        label_col: Name of the label column\n",
    "        prediction_col: Name of the prediction column\n",
    "    \"\"\"\n",
    "    pred_data = predictions.select(label_col, prediction_col).toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(pred_data[label_col], pred_data[prediction_col], alpha=0.5)\n",
    "    plt.plot([pred_data[label_col].min(), pred_data[label_col].max()],\n",
    "             [pred_data[label_col].min(), pred_data[label_col].max()],\n",
    "             'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_model_metrics(metrics, model_name, output_path):\n",
    "    \"\"\"\n",
    "    Save model evaluation metrics to a file.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "        model_name: Name of the model\n",
    "        output_path: Path to save the metrics\n",
    "    \"\"\"\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(f\"\\n{model_name} Performance Metrics:\\n\")\n",
    "        for metric, value in metrics.items():\n",
    "            f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Analyze missing values\n",
    "plot_missing_values(df)\n",
    "\n",
    "# Get detailed missing value statistics\n",
    "missing_stats = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()\n",
    "missing_stats = missing_stats.T.reset_index()\n",
    "missing_stats.columns = ['Column', 'Missing Count']\n",
    "missing_stats['Missing Percentage'] = (missing_stats['Missing Count'] / df.count()) * 100\n",
    "missing_stats = missing_stats.sort_values('Missing Percentage', ascending=False)\n",
    "\n",
    "print(\"\\nDetailed Missing Value Analysis:\")\n",
    "print(missing_stats[missing_stats['Missing Percentage'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import NumericType, StringType, BooleanType, IntegerType\n",
    "\n",
    "# Assuming `df` is your DataFrame\n",
    "df_schema = df.dtypes\n",
    "\n",
    "# Lists to hold the column names\n",
    "numerical_columns = []\n",
    "categorical_columns = []\n",
    "binary_columns = []\n",
    "\n",
    "# Loop through the schema and classify the columns\n",
    "for column, dtype in df_schema:\n",
    "    if isinstance(df.schema[column].dataType, NumericType):\n",
    "        numerical_columns.append(column)\n",
    "    elif isinstance(df.schema[column].dataType, StringType):\n",
    "        categorical_columns.append(column)\n",
    "    elif isinstance(df.schema[column].dataType, BooleanType):\n",
    "        binary_columns.append(column)\n",
    "\n",
    "# Show the results\n",
    "print(\"Numerical Columns:\", numerical_columns)\n",
    "print(\"Categorical Columns:\", categorical_columns)\n",
    "print(\"Binary Columns:\", binary_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the currency to EGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the exchange rate\n",
    "exchange_rate = 0.00038  # 1 Iranian Rial = 0.00038 Egyptian Pound\n",
    "\n",
    "# Update the 'price', 'primaryPrice', 'vendor_freeShippingToIran', and 'vendor_freeShippingToSameCity' columns\n",
    "df = df.withColumn('price', col('price') * exchange_rate) \\\n",
    "       .withColumn('primaryPrice', col('primaryPrice') * exchange_rate) \\\n",
    "       .withColumn('vendor_freeShippingToIran', col('vendor_freeShippingToIran') * exchange_rate) \\\n",
    "       .withColumn('vendor_freeShippingToSameCity', col('vendor_freeShippingToSameCity') * exchange_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in 'sales_count_week'\n",
    "sales_count_week_distribution = df.groupBy('sales_count_week').count().orderBy('sales_count_week')\n",
    "\n",
    "# Show the result\n",
    "sales_count_week_distribution.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in 'sales_count_week'\n",
    "sales_count_week_distribution = df.groupBy('_score').count().orderBy('_score')\n",
    "\n",
    "# Show the result\n",
    "sales_count_week_distribution.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique product names\n",
    "unique_product_count = df.select('name').distinct().count()\n",
    "\n",
    "print(f\"Number of unique product names: {unique_product_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for numerical features\n",
    "numeric_stats = df.select(numerical_columns).describe().toPandas()\n",
    "print(\"Numerical Features Statistics:\")\n",
    "display(numeric_stats)\n",
    "\n",
    "# Display distribution plots for numerical features\n",
    "# numerical_data = df.select(numerical_columns).toPandas()\n",
    "\n",
    "# fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "# axes = axes.ravel()\n",
    "\n",
    "# for idx, col in enumerate(numerical_columns):\n",
    "#     if idx < len(axes):\n",
    "#         sns.histplot(data=numerical_data, x=col, ax=axes[idx])\n",
    "#         axes[idx].set_title(f'Distribution of {col}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for all columns\n",
    "df_summary = df.describe()\n",
    "\n",
    "# Filter for categorical (string) columns\n",
    "categorical_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "# Get summary statistics only for categorical columns\n",
    "df_categorical_summary = df.select(categorical_columns).describe()\n",
    "\n",
    "# Show the result\n",
    "df_categorical_summary.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sales Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between sales, stock, and price\n",
    "sales_analysis = df.select('_score', 'stock', 'price', 'primaryPrice').toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "# Sales vs Stock\n",
    "sns.scatterplot(data=sales_analysis, x='stock', y='_score', ax=axes[0,0], alpha=0.5)\n",
    "axes[0,0].set_title('Score vs Stock')\n",
    "\n",
    "# Sales vs Price\n",
    "sns.scatterplot(data=sales_analysis, x='price', y='_score', ax=axes[0,1], alpha=0.5)\n",
    "axes[0,1].set_title('Score vs Price')\n",
    "\n",
    "# Sales vs Primary Price\n",
    "sns.scatterplot(data=sales_analysis, x='primaryPrice', y='_score', ax=axes[1,0], alpha=0.5)\n",
    "axes[1,0].set_title('Score vs Primary Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique categories\n",
    "unique_categories = df.select('categoryTitle').distinct().count()\n",
    "\n",
    "print(f\"Number of unique categories: {unique_categories}\")\n",
    "\n",
    "# Get all unique category titles\n",
    "unique_category_titles = df.select('categoryTitle').distinct().collect()\n",
    "\n",
    "# Display the unique categories\n",
    "for category in unique_category_titles:\n",
    "    print(category['categoryTitle'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation dictionary for category titles\n",
    "# Updated Translation dictionary for category titles\n",
    "category_translation = {\n",
    "    'گام شمار': 'Pedometer',\n",
    "    'کتاب چاپی': 'Printed Books',\n",
    "    'طلق موتور': 'Motor Oil',\n",
    "    'کفش و دمپایی زنانه': 'Women\\'s Shoes and Slippers',\n",
    "    'غذای ماهی و میگو': 'Fish and Shrimp Food',\n",
    "    'کفش، دمپایی مردانه': 'Men\\'s Shoes and Slippers',\n",
    "    'بذر و تخم گیاهان': 'Seeds and Plant Seeds',\n",
    "    'ذخیره سازی مبتنی بر نوار': 'Tape-based Storage',\n",
    "    'عطر و ادکلن زنانه و مردانه': 'Women\\'s and Men\\'s Perfume and Cologne',\n",
    "    'سایر': 'Other',\n",
    "    'ادویه': 'Spices',\n",
    "    'زیورآلات زنانه': 'Women\\'s Jewelry',\n",
    "    'لباس زیر زنانه': 'Women\\'s Lingerie',\n",
    "    'گیاهان دارویی': 'Medicinal Plants',\n",
    "    'مانتو و تونیک': 'Manto and Tunic'\n",
    "}\n",
    "\n",
    "\n",
    "# Analyze sales by category with product count threshold\n",
    "category_analysis = df.groupBy('categoryTitle').agg(\n",
    "    F.avg('_score').alias('avg_score'),\n",
    "    F.count('*').alias('product_count'),\n",
    "    F.avg('price').alias('avg_price'),\n",
    "    F.avg('rating_average').alias('avg_rating')\n",
    ").toPandas()\n",
    "\n",
    "# Apply threshold to filter out categories with less than 5000 products\n",
    "category_analysis = category_analysis[category_analysis['product_count'] >= 5000]\n",
    "\n",
    "# Translate category titles\n",
    "category_analysis['categoryTitle'] = category_analysis['categoryTitle'].map(category_translation).fillna(category_analysis['categoryTitle'])\n",
    "\n",
    "# Plot top categories by average score\n",
    "plt.figure(figsize=(15, 6))\n",
    "top_categories = category_analysis.nlargest(10, 'avg_score')\n",
    "sns.barplot(data=top_categories, x='categoryTitle', y='avg_score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Average Weekly Sales by Top 10 Categories (with 5000+ Products)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display category statistics\n",
    "print(\"\\nCategory Statistics:\")\n",
    "display(category_analysis.sort_values('avg_score', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Shipping and Delivery Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Analyze impact of free shipping and delivery options\n",
    "shipping_features = ['isFreeShipping', 'has_delivery',\n",
    "                     'vendor_freeShippingToIran', \n",
    "                     'vendor_freeShippingToSameCity']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(shipping_features):\n",
    "    # Group by feature and calculate the stats\n",
    "    shipping_stats = df.groupBy(feature) \\\n",
    "        .agg(F.avg('_score').alias('avg_score'),\n",
    "             F.count('*').alias('count')) \\\n",
    "        .toPandas()\n",
    "    \n",
    "    # Handle binning for continuous variables\n",
    "    if feature in ['vendor_freeShippingToIran', 'vendor_freeShippingToSameCity']:\n",
    "        # Get min and max values\n",
    "        min_val = max(0, shipping_stats[feature].min())  # Ensure minimum is 0\n",
    "        max_val = shipping_stats[feature].max()\n",
    "        \n",
    "        # Create 10 bins starting from 0\n",
    "        bins = np.linspace(min_val, max_val, 11)  # 11 edges to create 10 bins\n",
    "        \n",
    "        # Create the binned column\n",
    "        shipping_stats[feature + '_binned'] = pd.cut(shipping_stats[feature], \n",
    "                                                    bins=bins,\n",
    "                                                    include_lowest=True)\n",
    "        \n",
    "        # Update the plot x-axis with binned feature\n",
    "        x_feature = feature + '_binned'\n",
    "    else:\n",
    "        # For other features (like 'isFreeShipping'), just use them directly\n",
    "        x_feature = feature\n",
    "    \n",
    "    # Handle missing values\n",
    "    shipping_stats = shipping_stats.dropna(subset=[x_feature])\n",
    "    \n",
    "    # Plot the barplot\n",
    "    sns.barplot(data=shipping_stats, x=x_feature, y='avg_score', ax=axes[idx])\n",
    "    axes[idx].set_title(f'Average Scores by {feature}')\n",
    "    axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vendor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Determine which vendors receive the highest customer satisfaction ratings. \n",
    "Are there specific vendors that consistently receive positive feedback from \n",
    "customers? \n",
    "rating_average, \n",
    "rating_count, \n",
    "vendor_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vendor performance\n",
    "vendor_analysis = df.groupBy('vendor_name') \\\n",
    "    .agg(F.avg('_score').alias('avg_score'),\n",
    "         F.avg('rating_average').alias('avg_rating'),\n",
    "         F.count('*').alias('product_count')) \\\n",
    "    .filter('product_count >= 10') \\\n",
    "    .toPandas()\n",
    "\n",
    "# Plot vendor performance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vendor_analysis['avg_rating'], \n",
    "           vendor_analysis['avg_score'],\n",
    "           s=vendor_analysis['product_count'],\n",
    "           alpha=0.6)\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Average Weekly Sales')\n",
    "plt.title('Vendor Performance: Sales vs Ratings (size = product count)')\n",
    "\n",
    "# Add annotations for top performers\n",
    "top_vendors = vendor_analysis.nlargest(5, 'avg_score')\n",
    "for _, vendor in top_vendors.iterrows():\n",
    "    plt.annotate(vendor['vendor_name'],\n",
    "                 (vendor['avg_rating'], vendor['avg_sales']))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot correlation matrix\n",
    "plot_correlation_matrix(df, numerical_columns)\n",
    "\n",
    "# Calculate detailed correlations with sales\n",
    "correlations = []\n",
    "for feature in numerical_columns:\n",
    "    correlation = df.stat.corr('sales_count_week', feature)\n",
    "    correlations.append({\n",
    "        'feature': feature,\n",
    "        'correlation': correlation\n",
    "    })\n",
    "\n",
    "correlations_df = pd.DataFrame(correlations)\n",
    "correlations_df = correlations_df.sort_values('correlation', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nCorrelations with sales_count_week:\")\n",
    "display(correlations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots for numerical features\n",
    "plt.figure(figsize=(15, 6))\n",
    "numerical_data.boxplot(column=numerical_columns, figsize=(15, 6))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Box Plots of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detect outliers using Z-score method\n",
    "outlier_stats = {}\n",
    "for feature in numerical_columns:\n",
    "    df_with_outliers = detect_outliers(df, feature)\n",
    "    outlier_count = df_with_outliers.filter(col(f'{feature}_is_outlier')).count()\n",
    "    outlier_percentage = (outlier_count / df.count()) * 100\n",
    "    outlier_stats[feature] = {\n",
    "        'outlier_count': outlier_count,\n",
    "        'outlier_percentage': outlier_percentage\n",
    "    }\n",
    "\n",
    "print(\"\\nOutlier Statistics (|z-score| > 3):\")\n",
    "outlier_df = pd.DataFrame.from_dict(outlier_stats, orient='index')\n",
    "display(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis above, here are the key insights:\n",
    "\n",
    "1. **Missing Data**:\n",
    "   - [Will be filled based on actual analysis]\n",
    "\n",
    "2. **Sales Patterns**:\n",
    "   - Relationship between sales and stock levels\n",
    "   - Impact of pricing on sales\n",
    "   - Effect of discounts\n",
    "\n",
    "3. **Category Performance**:\n",
    "   - Top performing categories\n",
    "   - Category-wise pricing strategies\n",
    "\n",
    "4. **Shipping Impact**:\n",
    "   - Effect of free shipping on sales\n",
    "   - Delivery options influence\n",
    "\n",
    "5. **Vendor Analysis**:\n",
    "   - Top performing vendors\n",
    "   - Relationship between ratings and sales\n",
    "\n",
    "6. **Correlations**:\n",
    "   - Strong predictors of sales\n",
    "   - Feature relationships\n",
    "\n",
    "7. **Outliers**:\n",
    "   - Distribution of extreme values\n",
    "   - Impact on modeling strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
