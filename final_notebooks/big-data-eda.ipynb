{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be04790",
   "metadata": {
    "papermill": {
     "duration": 0.008037,
     "end_time": "2025-04-21T13:06:12.715637",
     "exception": false,
     "start_time": "2025-04-21T13:06:12.707600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# E-commerce Sales Prediction - Exploratory Data Analysis\n",
    "\n",
    "This notebook contains comprehensive exploratory data analysis for the e-commerce sales prediction project. We'll analyze various aspects of the data to understand patterns and relationships that can help in predicting weekly sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5f2b4",
   "metadata": {
    "papermill": {
     "duration": 0.007774,
     "end_time": "2025-04-21T13:06:12.731055",
     "exception": false,
     "start_time": "2025-04-21T13:06:12.723281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dba9a1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:06:12.746848Z",
     "iopub.status.busy": "2025-04-21T13:06:12.745932Z",
     "iopub.status.idle": "2025-04-21T13:09:42.731560Z",
     "shell.execute_reply": "2025-04-21T13:09:42.730394Z"
    },
    "papermill": {
     "duration": 209.996251,
     "end_time": "2025-04-21T13:09:42.734201",
     "exception": false,
     "start_time": "2025-04-21T13:06:12.737950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78323a636410>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/findspark/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78323a62bf90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/findspark/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78323a64afd0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/findspark/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78323a64b4d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/findspark/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78323a664850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/findspark/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement findspark (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for findspark\u001b[0m\u001b[31m\r\n",
      "\u001b[0mRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\r\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.3)\r\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.7.5)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.56.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.8)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (24.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (11.1.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.2.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.9.0.post0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (2.4.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy!=1.24.0,>=1.17->seaborn) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy!=1.24.0,>=1.17->seaborn) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy!=1.24.0,>=1.17->seaborn) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa13cc21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:09:42.752293Z",
     "iopub.status.busy": "2025-04-21T13:09:42.751034Z",
     "iopub.status.idle": "2025-04-21T13:09:42.833943Z",
     "shell.execute_reply": "2025-04-21T13:09:42.832596Z"
    },
    "papermill": {
     "duration": 0.093428,
     "end_time": "2025-04-21T13:09:42.835472",
     "exception": true,
     "start_time": "2025-04-21T13:09:42.742044",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/2682810582.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import required libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import findspark\n",
    "findspark.init\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when, count, mean, stddev\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalesPrediction_EDA\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv('/kaggle/input/basalam-comments-and-products/BaSalam.products.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Number of records: {df.count():,}\")\n",
    "print(f\"Number of features: {len(df.columns)}\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b028ba3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef881cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:45:27.636213Z",
     "iopub.status.busy": "2025-04-21T12:45:27.635325Z",
     "iopub.status.idle": "2025-04-21T12:51:42.771472Z",
     "shell.execute_reply": "2025-04-21T12:51:42.770380Z",
     "shell.execute_reply.started": "2025-04-21T12:45:27.636170Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "total_count = df.count()\n",
    "missing_counts = []\n",
    "\n",
    "for column in df.columns:\n",
    "    missing_count = df.filter(col(column).isNull()).count()\n",
    "    missing_percentage = (missing_count / total_count) * 100\n",
    "    missing_counts.append({\n",
    "        'column': column,\n",
    "        'missing_percentage': missing_percentage\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_counts)\n",
    "missing_df = missing_df.sort_values('missing_percentage', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1bdc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:51:42.772854Z",
     "iopub.status.busy": "2025-04-21T12:51:42.772573Z",
     "iopub.status.idle": "2025-04-21T12:52:03.854012Z",
     "shell.execute_reply": "2025-04-21T12:52:03.852730Z",
     "shell.execute_reply.started": "2025-04-21T12:51:42.772827Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.barh(missing_df['column'], missing_df['missing_percentage'])\n",
    "plt.xlabel('Missing Percentage')\n",
    "plt.title('Missing Values Analysis')\n",
    "plt.show()\n",
    "\n",
    "# Get detailed missing value statistics\n",
    "missing_stats = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()\n",
    "missing_stats = missing_stats.T.reset_index()\n",
    "missing_stats.columns = ['Column', 'Missing Count']\n",
    "missing_stats['Missing Percentage'] = (missing_stats['Missing Count'] / df.count()) * 100\n",
    "missing_stats = missing_stats.sort_values('Missing Percentage', ascending=False)\n",
    "\n",
    "print(\"\\nDetailed Missing Value Analysis:\")\n",
    "print(missing_stats[missing_stats['Missing Percentage'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e481cc2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b529e76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:52:03.856029Z",
     "iopub.status.busy": "2025-04-21T12:52:03.855707Z",
     "iopub.status.idle": "2025-04-21T12:52:03.864584Z",
     "shell.execute_reply": "2025-04-21T12:52:03.863324Z",
     "shell.execute_reply.started": "2025-04-21T12:52:03.855993Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import NumericType, StringType, BooleanType, IntegerType\n",
    "\n",
    "# Assuming `df` is your DataFrame\n",
    "df_schema = df.dtypes\n",
    "\n",
    "# Lists to hold the column names\n",
    "numerical_columns = []\n",
    "categorical_columns = []\n",
    "binary_columns = []\n",
    "\n",
    "# Loop through the schema and classify the columns\n",
    "for column, dtype in df_schema:\n",
    "    if isinstance(df.schema[column].dataType, NumericType):\n",
    "        numerical_columns.append(column)\n",
    "    elif isinstance(df.schema[column].dataType, StringType):\n",
    "        categorical_columns.append(column)\n",
    "    elif isinstance(df.schema[column].dataType, BooleanType):\n",
    "        binary_columns.append(column)\n",
    "\n",
    "# Show the results\n",
    "print(\"Numerical Columns:\", numerical_columns)\n",
    "print(\"Categorical Columns:\", categorical_columns)\n",
    "print(\"Binary Columns:\", binary_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce404df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:52:03.866524Z",
     "iopub.status.busy": "2025-04-21T12:52:03.866122Z",
     "iopub.status.idle": "2025-04-21T12:52:03.991369Z",
     "shell.execute_reply": "2025-04-21T12:52:03.990506Z",
     "shell.execute_reply.started": "2025-04-21T12:52:03.866487Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Changing the currency to EGP\n",
    "exchange_rate = 0.00038  # 1 Iranian Rial = 0.00038 Egyptian Pound\n",
    "\n",
    "# Update the 'price', 'primaryPrice', 'vendor_freeShippingToIran', and 'vendor_freeShippingToSameCity' columns\n",
    "df = df.withColumn('price', col('price') * exchange_rate) \\\n",
    "       .withColumn('primaryPrice', col('primaryPrice') * exchange_rate) \\\n",
    "       .withColumn('vendor_freeShippingToIran', col('vendor_freeShippingToIran') * exchange_rate) \\\n",
    "       .withColumn('vendor_freeShippingToSameCity', col('vendor_freeShippingToSameCity') * exchange_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6379d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:52:03.995554Z",
     "iopub.status.busy": "2025-04-21T12:52:03.995153Z",
     "iopub.status.idle": "2025-04-21T12:52:30.042756Z",
     "shell.execute_reply": "2025-04-21T12:52:30.041974Z",
     "shell.execute_reply.started": "2025-04-21T12:52:03.995519Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for numerical features\n",
    "numeric_stats = df.select(numerical_columns).describe().toPandas()\n",
    "print(\"Numerical Features Statistics:\")\n",
    "display(numeric_stats)\n",
    "\n",
    "# Display distribution plots for numerical features\n",
    "# numerical_data = df.select(numerical_columns).toPandas()\n",
    "\n",
    "# fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "# axes = axes.ravel()\n",
    "\n",
    "# for idx, col in enumerate(numerical_columns):\n",
    "#     if idx < len(axes):\n",
    "#         sns.histplot(data=numerical_data, x=col, ax=axes[idx])\n",
    "#         axes[idx].set_title(f'Distribution of {col}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2ce4c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Key Insights\n",
    "#### 1. Price vs Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede49a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:52:30.043743Z",
     "iopub.status.busy": "2025-04-21T12:52:30.043454Z",
     "iopub.status.idle": "2025-04-21T12:53:09.365825Z",
     "shell.execute_reply": "2025-04-21T12:53:09.364765Z",
     "shell.execute_reply.started": "2025-04-21T12:52:30.043706Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the number of bins you want\n",
    "num_bins = 10\n",
    "\n",
    "# Calculate bins for each price column\n",
    "price_bins = df.select(F.min(\"price\"), F.max(\"price\")).collect()\n",
    "min_price, max_price = price_bins[0][0], price_bins[0][1]\n",
    "price_bin_width = (max_price - min_price) / num_bins\n",
    "\n",
    "# Create binned columns\n",
    "df = df.withColumn(\"price_bin\", F.floor((F.col(\"price\") - min_price) / price_bin_width))\n",
    "# Aggregate average score by price bin\n",
    "price_score = df.groupBy(\"price_bin\").agg(\n",
    "    F.avg(\"_score\").alias(\"avg_score\"),\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ").orderBy(\"price_bin\")\n",
    "# Convert aggregated data to Pandas DataFrames\n",
    "price_pd = price_score.toPandas()\n",
    "# Calculate bin centers\n",
    "price_pd['bin_center'] = min_price + (price_pd['price_bin'] + 0.5) * price_bin_width\n",
    "\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# First convert the necessary columns to Pandas\n",
    "pdf = df.select(\"price\", \"_score\").toPandas()\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# 1. Price vs Score histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Create bins for price\n",
    "price_bins = np.linspace(pdf['price'].min(), pdf['price'].max(), num_bins + 1)\n",
    "pdf['price_bin'] = pd.cut(pdf['price'], bins=price_bins)\n",
    "\n",
    "# Calculate average score per bin\n",
    "price_stats = pdf.groupby('price_bin')['_score'].agg(['mean', 'count']).reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.bar(price_stats['price_bin'].apply(lambda x: x.mid), \n",
    "        price_stats['mean'], \n",
    "        width=(price_bins[1]-price_bins[0])*0.9,  # 90% of bin width for spacing\n",
    "        alpha=0.7)\n",
    "plt.title('Average Score by Price Bin')\n",
    "plt.xlabel('Price Range')\n",
    "plt.ylabel('Average Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. PrimaryPrice vs Score histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4d5ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 2. Discount Percentage vs score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef379d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:53:09.367251Z",
     "iopub.status.busy": "2025-04-21T12:53:09.366929Z",
     "iopub.status.idle": "2025-04-21T12:53:20.732927Z",
     "shell.execute_reply": "2025-04-21T12:53:20.731705Z",
     "shell.execute_reply.started": "2025-04-21T12:53:09.367221Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## 1. Calculate Discount Percentage\n",
    "# If primaryPrice is 0, set discount to 0, otherwise calculate (primaryPrice - price)/primaryPrice\n",
    "df = df.withColumn(\n",
    "    \"discount_pct\",\n",
    "    F.when(F.col(\"primaryPrice\") == 0, 0)\n",
    "     .otherwise((F.col(\"primaryPrice\") - F.col(\"price\")) / F.col(\"primaryPrice\") * 100\n",
    "))\n",
    "\n",
    "## 2. Filter out invalid discounts (optional)\n",
    "# Remove cases where discount is negative (price > primaryPrice) or >100%\n",
    "df = df.filter((F.col(\"discount_pct\") >= 0) & (F.col(\"discount_pct\") <= 100))\n",
    "\n",
    "## 3. Bin the discount percentages\n",
    "num_bins = 10  # You can adjust this\n",
    "df = df.withColumn(\"discount_bin\", F.floor(F.col(\"discount_pct\") / (100/num_bins)))\n",
    "\n",
    "## 4. Aggregate by bins\n",
    "discount_stats = df.groupBy(\"discount_bin\").agg(\n",
    "    F.avg(\"_score\").alias(\"avg_score\"),\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ").orderBy(\"discount_bin\")\n",
    "\n",
    "## 5. Convert to Pandas for plotting\n",
    "discount_pd = discount_stats.toPandas()\n",
    "\n",
    "# Calculate bin centers (middle of each range)\n",
    "discount_pd['bin_center'] = (discount_pd['discount_bin'] + 0.5) * (100/num_bins)\n",
    "\n",
    "## 6. Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Main bar plot\n",
    "bars = plt.bar(discount_pd['bin_center'], \n",
    "               discount_pd['avg_score'], \n",
    "               width=(100/num_bins)*0.8,\n",
    "               alpha=0.7,\n",
    "               color='green')\n",
    "\n",
    "# Add count labels if desired\n",
    "for bar, count in zip(bars, discount_pd['count']):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(count)}',\n",
    "             ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.title('Average Score by Discount Percentage')\n",
    "plt.xlabel('Discount Percentage (%)')\n",
    "plt.ylabel('Average Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(np.arange(0, 101, 10))\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc74008",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 3. Weight vs Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e4751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:53:20.734429Z",
     "iopub.status.busy": "2025-04-21T12:53:20.734004Z",
     "iopub.status.idle": "2025-04-21T12:53:49.529118Z",
     "shell.execute_reply": "2025-04-21T12:53:49.528102Z",
     "shell.execute_reply.started": "2025-04-21T12:53:20.734404Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate IQR for both weight and price\n",
    "quantiles = df.approxQuantile([\"weight\", \"price\"], [0.25, 0.75], 0.05)\n",
    "weight_q1, weight_q3 = quantiles[0][0], quantiles[0][1]\n",
    "price_q1, price_q3 = quantiles[1][0], quantiles[1][1]\n",
    "\n",
    "# Define bounds (1.5*IQR rule)\n",
    "weight_iqr = weight_q3 - weight_q1\n",
    "price_iqr = price_q3 - price_q1\n",
    "\n",
    "df_clean = df.filter(\n",
    "    (F.col(\"weight\") >= weight_q1 - 1.5*weight_iqr) &\n",
    "    (F.col(\"weight\") <= weight_q3 + 1.5*weight_iqr) &\n",
    "    (F.col(\"price\") >= price_q1 - 1.5*price_iqr) &\n",
    "    (F.col(\"price\") <= price_q3 + 1.5*price_iqr)\n",
    ")\n",
    "\n",
    "print(f\"Removed {df.count() - df_clean.count()} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de160e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:53:49.530634Z",
     "iopub.status.busy": "2025-04-21T12:53:49.530308Z",
     "iopub.status.idle": "2025-04-21T12:54:09.111014Z",
     "shell.execute_reply": "2025-04-21T12:54:09.109045Z",
     "shell.execute_reply.started": "2025-04-21T12:53:49.530599Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Re-bin the cleaned data\n",
    "weight_bins = df_clean.select(\n",
    "    F.min(\"weight\").alias(\"min_w\"),\n",
    "    F.max(\"weight\").alias(\"max_w\")\n",
    ").collect()\n",
    "min_w, max_w = weight_bins[0][0], weight_bins[0][1]\n",
    "bin_width = (max_w - min_w) / 20  # Adjust bin count as needed\n",
    "\n",
    "df_binned = df_clean.withColumn(\n",
    "    \"weight_bin\", \n",
    "    F.floor((F.col(\"weight\") - min_w) / bin_width)\n",
    ").groupBy(\"weight_bin\").agg(\n",
    "    F.avg(\"price\").alias(\"avg_price\"),\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ").orderBy(\"weight_bin\")\n",
    "\n",
    "hist_data = df_binned.toPandas()\n",
    "hist_data[\"bin_center\"] = min_w + (hist_data[\"weight_bin\"] + 0.5) * bin_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534c5b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:54:09.112214Z",
     "iopub.status.busy": "2025-04-21T12:54:09.111910Z",
     "iopub.status.idle": "2025-04-21T12:54:09.428650Z",
     "shell.execute_reply": "2025-04-21T12:54:09.427638Z",
     "shell.execute_reply.started": "2025-04-21T12:54:09.112171Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(\n",
    "    hist_data[\"bin_center\"], \n",
    "    hist_data[\"avg_price\"], \n",
    "    width=bin_width*0.8,\n",
    "    color=\"teal\",\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add count labels (only for bins with reasonable counts)\n",
    "for idx, row in hist_data.iterrows():\n",
    "    if row[\"count\"] > 10:  # Only label meaningful bins\n",
    "        plt.text(\n",
    "            row[\"bin_center\"], \n",
    "            row[\"avg_price\"] * 1.05,  # Position above bar\n",
    "            f\"n={int(row['count']):,}\",\n",
    "            ha=\"center\",\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "plt.title(\"Average Price by Weight Bin (Outliers Removed)\", fontsize=14)\n",
    "plt.xlabel(\"Weight (units)\", fontsize=12)\n",
    "plt.ylabel(\"Average Price\", fontsize=12)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# Set y-axis limit based on cleaned data\n",
    "plt.ylim(0, hist_data[\"avg_price\"].max() * 1.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619bba38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 4. isFreeShipping vs Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebb195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:54:09.430703Z",
     "iopub.status.busy": "2025-04-21T12:54:09.430347Z",
     "iopub.status.idle": "2025-04-21T12:54:21.413137Z",
     "shell.execute_reply": "2025-04-21T12:54:21.410563Z",
     "shell.execute_reply.started": "2025-04-21T12:54:09.430669Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate average scores by free shipping status\n",
    "score_comparison = df.groupBy(\"isFreeShipping\").agg(\n",
    "    F.avg(\"_score\").alias(\"average_score\"),\n",
    "    F.count(\"*\").alias(\"product_count\"),\n",
    "    F.stddev(\"_score\").alias(\"score_stddev\"),\n",
    "    F.percentile_approx(\"_score\", 0.5).alias(\"median_score\")\n",
    ").orderBy(\"isFreeShipping\")\n",
    "\n",
    "# Show results\n",
    "score_comparison.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14b8e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 5. has_variation vs Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770fec22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:54:21.414280Z",
     "iopub.status.busy": "2025-04-21T12:54:21.413963Z",
     "iopub.status.idle": "2025-04-21T12:54:33.431391Z",
     "shell.execute_reply": "2025-04-21T12:54:33.430286Z",
     "shell.execute_reply.started": "2025-04-21T12:54:21.414251Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate average scores by free shipping status\n",
    "score_comparison = df.groupBy(\"has_variation\").agg(\n",
    "    F.avg(\"_score\").alias(\"average_score\"),\n",
    "    F.count(\"*\").alias(\"product_count\"),\n",
    "    F.stddev(\"_score\").alias(\"score_stddev\"),\n",
    "    F.percentile_approx(\"_score\", 0.5).alias(\"median_score\")\n",
    ").orderBy(\"has_variation\")\n",
    "\n",
    "# Show results\n",
    "score_comparison.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d4c88",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 6. isFreeShipping vs Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de05e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:54:33.432875Z",
     "iopub.status.busy": "2025-04-21T12:54:33.432531Z",
     "iopub.status.idle": "2025-04-21T12:54:45.012808Z",
     "shell.execute_reply": "2025-04-21T12:54:45.011493Z",
     "shell.execute_reply.started": "2025-04-21T12:54:33.432853Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Weight vs isFreshipping\n",
    "score_comparison = df.groupBy(\"isFreeShipping\").agg(\n",
    "    F.avg(\"weight\").alias(\"average_weight\"),\n",
    "    F.count(\"*\").alias(\"product_count\"),\n",
    "    F.stddev(\"weight\").alias(\"weight_stddev\"),\n",
    "    F.percentile_approx(\"_score\", 0.5).alias(\"median_weight\")\n",
    ").orderBy(\"isFreeShipping\")\n",
    "\n",
    "# Show results\n",
    "score_comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e262e90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:54:45.014518Z",
     "iopub.status.busy": "2025-04-21T12:54:45.014130Z",
     "iopub.status.idle": "2025-04-21T12:54:55.078654Z",
     "shell.execute_reply": "2025-04-21T12:54:55.077853Z",
     "shell.execute_reply.started": "2025-04-21T12:54:45.014488Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Weight vs isFreshipping\n",
    "score_comparison = df.groupBy(\"isFreeShipping\").agg(\n",
    "    F.avg(\"price\").alias(\"average_price\"),\n",
    "    F.count(\"*\").alias(\"product_count\"),\n",
    ").orderBy(\"isFreeShipping\")\n",
    "\n",
    "# Show results\n",
    "score_comparison.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4aaec3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 7. has_delivery vs score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1af7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:54:55.079712Z",
     "iopub.status.busy": "2025-04-21T12:54:55.079376Z",
     "iopub.status.idle": "2025-04-21T12:55:06.823022Z",
     "shell.execute_reply": "2025-04-21T12:55:06.821736Z",
     "shell.execute_reply.started": "2025-04-21T12:54:55.079685Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate average scores by free shipping status\n",
    "score_comparison = df.groupBy(\"has_delivery\").agg(\n",
    "    F.avg(\"_score\").alias(\"average_score\"),\n",
    "    F.count(\"*\").alias(\"product_count\"),\n",
    "    F.stddev(\"_score\").alias(\"score_stddev\"),\n",
    "    F.percentile_approx(\"_score\", 0.5).alias(\"median_score\")\n",
    ").orderBy(\"has_delivery\")\n",
    "\n",
    "# Show results\n",
    "score_comparison.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb8201",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 8. top vendors that has highest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf85f11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:55:06.824548Z",
     "iopub.status.busy": "2025-04-21T12:55:06.824154Z",
     "iopub.status.idle": "2025-04-21T12:55:19.874867Z",
     "shell.execute_reply": "2025-04-21T12:55:19.873752Z",
     "shell.execute_reply.started": "2025-04-21T12:55:06.824454Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate vendor statistics\n",
    "vendor_stats = df.groupBy(\"vendor_id\", \"vendor_name\").agg(\n",
    "    F.avg(\"_score\").alias(\"avg_score\"),\n",
    "    F.count(\"*\").alias(\"product_count\"),\n",
    "    F.stddev(\"_score\").alias(\"score_stddev\"),\n",
    "    F.min(\"_score\").alias(\"min_score\"),\n",
    "    F.max(\"_score\").alias(\"max_score\")\n",
    ").filter(F.col(\"product_count\") > 10)  # Only vendors with sufficient products\n",
    "\n",
    "# Add consistency metric (lower stddev = more consistent)\n",
    "vendor_stats = vendor_stats.withColumn(\n",
    "    \"consistency\",\n",
    "    1/F.col(\"score_stddev\")  # Higher value = more consistent\n",
    ")\n",
    "\n",
    "# Rank vendors by average score\n",
    "window = Window.orderBy(F.desc(\"avg_score\"))\n",
    "vendor_stats = vendor_stats.withColumn(\"rank\", F.rank().over(window))\n",
    "\n",
    "# Show top 20 vendors\n",
    "top_vendors = vendor_stats.filter(F.col(\"rank\") <= 20).orderBy(\"rank\")\n",
    "top_vendors.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ed8f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:55:19.876384Z",
     "iopub.status.busy": "2025-04-21T12:55:19.875990Z",
     "iopub.status.idle": "2025-04-21T12:55:34.028593Z",
     "shell.execute_reply": "2025-04-21T12:55:34.027552Z",
     "shell.execute_reply.started": "2025-04-21T12:55:19.876345Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "top_vendors_pd = top_vendors.toPandas()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot 1: Top Vendors by Average Score\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(\n",
    "    x=\"avg_score\", \n",
    "    y=\"vendor_name\", \n",
    "    data=top_vendors_pd,\n",
    "    palette=\"viridis\",\n",
    "    hue=\"product_count\"\n",
    ")\n",
    "plt.title(\"Top Vendors by Average Score (Bubble Size = Product Count)\")\n",
    "plt.xlabel(\"Average Score\")\n",
    "plt.ylabel(\"Vendor Name\")\n",
    "plt.xlim(top_vendors_pd[\"avg_score\"].min()*0.98, 5.0)  # Assuming 5-point scale\n",
    "\n",
    "# Plot 2: Score Consistency\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.scatterplot(\n",
    "    x=\"avg_score\",\n",
    "    y=\"score_stddev\",\n",
    "    size=\"product_count\",\n",
    "    hue=\"vendor_name\",\n",
    "    data=top_vendors_pd,\n",
    "    legend=False,\n",
    "    sizes=(40, 400)\n",
    ")\n",
    "\n",
    "plt.title(\"Score Consistency (Lower StdDev = More Consistent)\")\n",
    "plt.xlabel(\"Average Score\")\n",
    "plt.ylabel(\"Score Standard Deviation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5bc43",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 8. Top Categories that has highest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacca167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:55:34.029932Z",
     "iopub.status.busy": "2025-04-21T12:55:34.029627Z",
     "iopub.status.idle": "2025-04-21T12:55:44.648806Z",
     "shell.execute_reply": "2025-04-21T12:55:44.647920Z",
     "shell.execute_reply.started": "2025-04-21T12:55:34.029903Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Translation dictionary for category titles\n",
    "# Updated Translation dictionary for category titles\n",
    "category_translation = {\n",
    "    'گام شمار': 'Pedometer',\n",
    "    'کتاب چاپی': 'Printed Books',\n",
    "    'طلق موتور': 'Motor Oil',\n",
    "    'کفش و دمپایی زنانه': 'Women\\'s Shoes and Slippers',\n",
    "    'غذای ماهی و میگو': 'Fish and Shrimp Food',\n",
    "    'کفش، دمپایی مردانه': 'Men\\'s Shoes and Slippers',\n",
    "    'بذر و تخم گیاهان': 'Seeds and Plant Seeds',\n",
    "    'ذخیره سازی مبتنی بر نوار': 'Tape-based Storage',\n",
    "    'عطر و ادکلن زنانه و مردانه': 'Women\\'s and Men\\'s Perfume and Cologne',\n",
    "    'سایر': 'Other',\n",
    "    'ادویه': 'Spices',\n",
    "    'زیورآلات زنانه': 'Women\\'s Jewelry',\n",
    "    'لباس زیر زنانه': 'Women\\'s Lingerie',\n",
    "    'گیاهان دارویی': 'Medicinal Plants',\n",
    "    'مانتو و تونیک': 'Manto and Tunic'\n",
    "}\n",
    "\n",
    "\n",
    "# Analyze sales by category with product count threshold\n",
    "category_analysis = df.groupBy('categoryTitle').agg(\n",
    "    F.avg('_score').alias('avg_score'),\n",
    "    F.count('*').alias('product_count'),\n",
    "    F.avg('price').alias('avg_price'),\n",
    "    F.avg('rating_average').alias('avg_rating')\n",
    ").toPandas()\n",
    "\n",
    "# Apply threshold to filter out categories with less than 5000 products\n",
    "category_analysis = category_analysis[category_analysis['product_count'] >= 5000]\n",
    "\n",
    "# Translate category titles\n",
    "category_analysis['categoryTitle'] = category_analysis['categoryTitle'].map(category_translation).fillna(category_analysis['categoryTitle'])\n",
    "\n",
    "# Plot top categories by average score\n",
    "plt.figure(figsize=(15, 6))\n",
    "top_categories = category_analysis.nlargest(10, 'avg_score')\n",
    "sns.barplot(data=top_categories, x='categoryTitle', y='avg_score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Average Score by Top 10 Categories (with 5000+ Products)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display category statistics\n",
    "print(\"\\nCategory Statistics:\")\n",
    "display(category_analysis.sort_values('avg_score', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51186635",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 9. vendor city vs score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce91ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:55:44.650024Z",
     "iopub.status.busy": "2025-04-21T12:55:44.649751Z",
     "iopub.status.idle": "2025-04-21T12:55:44.793636Z",
     "shell.execute_reply": "2025-04-21T12:55:44.792624Z",
     "shell.execute_reply.started": "2025-04-21T12:55:44.650004Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate city statistics (only for cities with sufficient vendors)\n",
    "city_stats = df.groupBy(\"vendor_cityId\", \"vendor_owner_city\").agg(\n",
    "    F.avg(\"_score\").alias(\"avg_score\"),\n",
    "    F.countDistinct(\"vendor_id\").alias(\"vendor_count\"),\n",
    "    F.count(\"*\").alias(\"product_count\"),\n",
    "    F.stddev(\"_score\").alias(\"score_stddev\")\n",
    ").filter(F.col(\"product_count\") >= 10)  # Only cities with 5+ vendors\n",
    "\n",
    "# Rank cities by average score\n",
    "window = Window.partitionBy().orderBy(F.desc(\"avg_score\"))  # Explicit empty partition\n",
    "city_stats = city_stats.withColumn(\"rank\", F.rank().over(window))\n",
    "\n",
    "# Show top/bottom 10 cities\n",
    "display(city_stats.orderBy(\"rank\").limit(10))\n",
    "display(city_stats.orderBy(F.desc(\"rank\")).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39a62c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:55:44.800188Z",
     "iopub.status.busy": "2025-04-21T12:55:44.799859Z",
     "iopub.status.idle": "2025-04-21T12:55:57.082111Z",
     "shell.execute_reply": "2025-04-21T12:55:57.081155Z",
     "shell.execute_reply.started": "2025-04-21T12:55:44.800157Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert top cities to Pandas\n",
    "top_cities_pd = city_stats.filter(F.col(\"rank\") <= 15).toPandas()\n",
    "\n",
    "# Plot: Bar Chart of Average Scores for Top Cities\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    x=\"vendor_owner_city\",\n",
    "    y=\"avg_score\",\n",
    "    data=top_cities_pd,\n",
    "    palette=\"viridis\",\n",
    "    order=top_cities_pd.sort_values(\"avg_score\", ascending=False)[\"vendor_owner_city\"]\n",
    ")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Top Cities by Average Product Score\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Average Score\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1681313",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 10. Determine which vendors receive the highest customer satisfaction ratings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1e947",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Adjusted Rating= ((avg_rating×rating_count)+(global_mean×min_samples)) / (rating_count+min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a2072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:55:57.083546Z",
     "iopub.status.busy": "2025-04-21T12:55:57.083283Z",
     "iopub.status.idle": "2025-04-21T12:56:07.276631Z",
     "shell.execute_reply": "2025-04-21T12:56:07.275480Z",
     "shell.execute_reply.started": "2025-04-21T12:55:57.083527Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate global mean and minimum sample threshold\n",
    "global_mean = df.select(F.avg(\"rating_average\")).first()[0]\n",
    "min_samples = 100  # Adjust based on your data\n",
    "\n",
    "vendor_stats = df.groupBy(\"vendor_id\", \"vendor_name\").agg(\n",
    "    F.avg(\"rating_average\").alias(\"avg_rating\"),\n",
    "    F.sum(\"rating_count\").alias(\"total_ratings\")\n",
    ").filter(F.col(\"total_ratings\") > min_samples)\n",
    "\n",
    "# Apply Bayesian Average\n",
    "vendor_stats = vendor_stats.withColumn(\n",
    "    \"adjusted_rating\",\n",
    "    (F.col(\"avg_rating\") * F.col(\"total_ratings\") + global_mean * min_samples) / \n",
    "    (F.col(\"total_ratings\") + min_samples)\n",
    ")\n",
    "\n",
    "# Rank by adjusted rating\n",
    "window = Window.orderBy(F.desc(\"adjusted_rating\"))\n",
    "vendor_stats = vendor_stats.withColumn(\"rank\", F.rank().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592877d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:56:07.278536Z",
     "iopub.status.busy": "2025-04-21T12:56:07.278085Z",
     "iopub.status.idle": "2025-04-21T12:56:18.834320Z",
     "shell.execute_reply": "2025-04-21T12:56:18.833002Z",
     "shell.execute_reply.started": "2025-04-21T12:56:07.278503Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_vendors_pd = vendor_stats.orderBy(\"rank\").limit(10).toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=\"vendor_name\",\n",
    "    y=\"adjusted_rating\",  # or \"wilson_score\"/\"weighted_score\"\n",
    "    data=top_vendors_pd,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top Vendors by Bayesian equation\")\n",
    "plt.ylabel(\"Weighted Average\")\n",
    "plt.xlabel(\"Vendor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56dbefa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 11. score vs average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2a3f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:56:18.836254Z",
     "iopub.status.busy": "2025-04-21T12:56:18.835958Z",
     "iopub.status.idle": "2025-04-21T12:56:29.266710Z",
     "shell.execute_reply": "2025-04-21T12:56:29.264983Z",
     "shell.execute_reply.started": "2025-04-21T12:56:18.836234Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Group by exact rating values (0-5)\n",
    "rating_analysis = df.groupBy(\"rating_average\") \\\n",
    "    .agg(\n",
    "        F.avg(\"_score\").alias(\"avg_score\"),\n",
    "        F.count(\"*\").alias(\"product_count\"),\n",
    "        F.stddev(\"_score\").alias(\"score_stddev\")\n",
    "    ) \\\n",
    "    .filter(F.col(\"rating_average\").isin([0, 1, 2, 3, 4, 5])) \\\n",
    "    .orderBy(\"rating_average\") \\\n",
    "    .toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(\n",
    "    rating_analysis[\"rating_average\"].astype(str),  # Convert ratings to strings for labels\n",
    "    rating_analysis[\"avg_score\"],\n",
    "    yerr=rating_analysis[\"score_stddev\"],  # Show standard deviation as error bars\n",
    "    capsize=5,\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "# Annotate bars with product counts\n",
    "for i, row in rating_analysis.iterrows():\n",
    "    plt.text(\n",
    "        i,\n",
    "        row[\"avg_score\"] + 0.5,  # Position above bar\n",
    "        f\"n={row['product_count']:,}\",\n",
    "        ha=\"center\",\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Exact Rating (0-5 scale)\")\n",
    "plt.ylabel(\"Average Score\")\n",
    "plt.title(\"Average Score by Exact Rating Value\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff777c4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 12. score vs sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66fe5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:56:29.268088Z",
     "iopub.status.busy": "2025-04-21T12:56:29.267812Z",
     "iopub.status.idle": "2025-04-21T12:57:09.270701Z",
     "shell.execute_reply": "2025-04-21T12:57:09.269596Z",
     "shell.execute_reply.started": "2025-04-21T12:56:29.268068Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the number of bins you want\n",
    "num_bins = 10\n",
    "\n",
    "# Calculate bins for each sales column\n",
    "sales_bins = df.select(F.min(\"sales_count_week\"), F.max(\"sales_count_week\")).collect()\n",
    "min_sales, max_sales = sales_bins[0][0], sales_bins[0][1]\n",
    "sales_bin_width = (max_sales - min_sales) / num_bins\n",
    "\n",
    "# Create binned columns\n",
    "df = df.withColumn(\"sales_bin\", F.floor((F.col(\"sales_count_week\") - min_sales) / sales_bin_width))\n",
    "# Aggregate average score by sales bin\n",
    "sales_score = df.groupBy(\"sales_bin\").agg(\n",
    "    F.avg(\"_score\").alias(\"avg_score\"),\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ").orderBy(\"sales_bin\")\n",
    "# Convert aggregated data to Pandas DataFrames\n",
    "sales_pd = sales_score.toPandas()\n",
    "# Calculate bin centers\n",
    "sales_pd['bin_center'] = min_sales + (sales_pd['sales_bin'] + 0.5) * sales_bin_width\n",
    "\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# First convert the necessary columns to Pandas\n",
    "pdf = df.select(\"sales_count_week\", \"_score\").toPandas()\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# 1. Sales vs Score histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Create bins for Sales\n",
    "sales_bins = np.linspace(pdf['sales_count_week'].min(), pdf['sales_count_week'].max(), num_bins + 1)\n",
    "pdf['sales_bin'] = pd.cut(pdf['sales_count_week'], bins=sales_bins)\n",
    "\n",
    "# Calculate average score per bin\n",
    "sales_stats = pdf.groupby('sales_bin')['_score'].agg(['mean', 'count']).reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.bar(sales_stats['sales_bin'].apply(lambda x: x.mid), \n",
    "        sales_stats['mean'], \n",
    "        width=(sales_bins[1]-sales_bins[0])*0.9,  # 90% of bin width for spacing\n",
    "        alpha=0.7)\n",
    "plt.title('Average Score by Sales Bin')\n",
    "plt.xlabel('Sales Range')\n",
    "plt.ylabel('Average Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. Sales vs Score histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00f129",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7f094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:57:09.272009Z",
     "iopub.status.busy": "2025-04-21T12:57:09.271694Z",
     "iopub.status.idle": "2025-04-21T12:58:12.789340Z",
     "shell.execute_reply": "2025-04-21T12:58:12.788141Z",
     "shell.execute_reply.started": "2025-04-21T12:57:09.271980Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and plot correlation matrix\n",
    "\n",
    "def plot_correlation_matrix(df, columns):\n",
    "    \"\"\"\n",
    "    Plot correlation matrix for specified columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        columns: List of column names to include in correlation matrix\n",
    "    \"\"\"\n",
    "    correlation_data = df.select(columns).toPandas()\n",
    "    correlation_matrix = correlation_data.corr()\n",
    "    \n",
    "    plt.figure(figsize=(25, 15))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Define binary columns\n",
    "binary_columns = ['has_delivery', 'has_variation', 'vendor_has_delivery', 'isFreeShipping', 'IsAvailable', 'IsSaleable']\n",
    "\n",
    "# Combine numerical and binary features\n",
    "all_features = numerical_columns + binary_columns\n",
    "\n",
    "# Plot correlation matrix with all features\n",
    "plot_correlation_matrix(df, all_features)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4956094,
     "sourceId": 8343753,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 215.588544,
   "end_time": "2025-04-21T13:09:43.262076",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-21T13:06:07.673532",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
